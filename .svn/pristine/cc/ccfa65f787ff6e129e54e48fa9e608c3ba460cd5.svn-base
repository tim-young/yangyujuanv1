package com.yangyujuan.lucene;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.cjk.CJKAnalyzer;
import org.apache.lucene.analysis.core.SimpleAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer; 
import org.apache.lucene.document.Document; 
import org.apache.lucene.document.Field;
import org.apache.lucene.document.FieldType;
import org.apache.lucene.document.IntField;
import org.apache.lucene.document.StringField;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.DocsEnum;
import org.apache.lucene.index.Fields;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.Terms;
import org.apache.lucene.index.TermsEnum;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.DocIdSetIterator;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.TopScoreDocCollector;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.Version;
import org.jsoup.Jsoup;

//import jeasy.analysis.MMAnalyzer;

import com.yangyujuan.jdbc.dao.DaoFactory;
import com.yangyujuan.jdbc.dao.NewsDao;
import com.yangyujuan.jdbc.dao.NewsFilter;
import com.yangyujuan.jdbc.domain.News;
import com.yangyujuan.util.PropertiesUtil;

import net.sf.json.JSONArray;
import net.sf.json.JSONObject;



public class LuceneService {

	public void createIndex() throws IOException {
		 IndexWriter writer = null;  
	        try {  
	        	String indexPath = PropertiesUtil.get("indexPath");
	            // Directory directory = new RAMDirectory();  
	            Directory directory = FSDirectory.open(Paths.get(indexPath));  
//	            Analyzer analyzer = new StandardAnalyzer();  
//	            Analyzer analyzer = new MMAnalyzer();//中文分词器
	            Analyzer analyzer = new org.apache.lucene.analysis.core.SimpleAnalyzer();
//	            Analyzer analyzer =  new CJKAnalyzer();
	            IndexWriterConfig iwc = new IndexWriterConfig(analyzer);  
	            writer = new IndexWriter(directory, iwc);  
	            Document document = null;  
	            
	            NewsDao newsDao = DaoFactory.getInstance().getUserDao();
	    		ArrayList<News> list = newsDao.getAllNewsList();
	    		FieldType type = new FieldType();
	    		type.setIndexOptions(org.apache.lucene.index.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
	    		type.setTokenized(true);
	    		type.setStored(true);
	    		type.setStoreTermVectors(true);
	    		
	    		for(News news : list){
	    			if(!NewsFilter.isRightIndex(news)){
	    				continue;
	    			}
	    			System.out.println("title:" + news.getTitle());  
	                document = new Document();  
//	                Field field = new Field("body", Jsoup.parse(news.getBodytext()).text(), type);
//	                document.add(new IntField("id", news.getId(), Field.Store.YES));
//	                document.add(new StringField("title", news.getTitle(), Field.Store.YES));
//	                document.add(new StringField("source", news.getSource(), Field.Store.YES));  
//	                document.add(new StringField("path", news.getPubTime(), Field.Store.YES));
	                //由于正文是带html标签的字符串，所有用Jsoup去掉标签
//	                document.add(new TextField("bodytext", Jsoup.parse(news.getBodytext()).text(), Field.Store.YES));
//	                document.add(new TextField("bodytexthtml", news.getBodytext(), Field.Store.NO));
	                document.add(new Field("id", String.valueOf(news.getId()), type));
	                document.add(new Field("title", news.getTitle(), type));
	                document.add(new Field("source", news.getSource(), type));  
	                document.add(new Field("path", news.getPubTime(), type));
	                document.add(new Field("bodytext", Jsoup.parse(news.getBodytext()).text(), type));
	                
	                writer.addDocument(document);  
	    		}
	        } catch (IOException e) {  
	            // TODO Auto-generated catch block  
	            e.printStackTrace();  
	        } finally {  
	            if (writer != null) {  
	                try {  
	                    writer.close();  
	                } catch (IOException e) {  
	                    // TODO Auto-generated catch block  
	                    e.printStackTrace();  
	                }  
	            }  
	        } 
	}

	public ArrayList<News> search(String keyword) throws ParseException{
	    ArrayList<News> ls = new ArrayList<News>();
	    
	    String indexPath = PropertiesUtil.get("indexPath");
        // Directory directory = new RAMDirectory();  
         
		try {
			Directory directory = FSDirectory.open(Paths.get(indexPath));
//			Analyzer analyzer = new StandardAnalyzer();  
//			Analyzer analyzer = new MMAnalyzer();//中文分词器
			Analyzer analyzer = new SimpleAnalyzer();
			DirectoryReader ireader = DirectoryReader.open(directory);
			
			IndexSearcher isearcher = new IndexSearcher(ireader);
			QueryParser parser = new QueryParser("bodytext", analyzer);
			Query query = parser.parse(keyword);
			ScoreDoc[] hits = isearcher.search(query, 10).scoreDocs;
			System.out.println(hits.length);
			for (int i = 0; i < hits.length; i++) {
				Document hitDoc = isearcher.doc(hits[i].doc);
				ls.add(document2News(hitDoc));
			}
			ireader.close();
			directory.close();
		} catch (IOException e) {
			e.printStackTrace();
		}  
		return ls;
	}
	
	public News document2News(Document document){
		News  model = new News();
		model.setId(Integer.parseInt(document.get("id")));
		model.setTitle(document.get("title"));
		model.setSource(document.get("source"));
		model.setPubTime(document.get("pubTime"));
		model.setBodytext(document.get("bodytext"));
		return model;
	}
	
	public void addIndex(News news){
		
	}
	
	public void getIndexMate(){
		 try {
			 	JSONObject jsonall = new JSONObject();
			 	ArrayList<HashMap> jsonlist = new ArrayList<HashMap>();
			 	
			 	String indexPath = PropertiesUtil.get("indexPath");
	            // Directory directory = new RAMDirectory();  
	            Directory directory = FSDirectory.open(Paths.get(indexPath));  
//	            Directory directroy = FSDirectory.open(new File(
//	                    INDEX_PATH));
	            IndexReader reader = DirectoryReader.open(directory);
	            //reader.numDocs()
	            for (int i = 0; i <5; i++) {
	                int docId = i;
	                
	                HashMap<String,Object> hashmap = new HashMap<String,Object>();
                	hashmap.put("name", i);
                	ArrayList<HashMap> termlist = new ArrayList<HashMap>();
                	
                	
	                System.out.println("第" + (i + 1) + "篇文档：");
	                Terms terms = reader.getTermVector(docId, "bodytext");
	                if (terms == null){
	                    continue;
	                }else{
	                	System.out.println(terms.toString());
	                }
	                TermsEnum termsEnum = terms.iterator();
	                BytesRef thisTerm = null;
	                while ((thisTerm = termsEnum.next()) != null) {
	                    String termText = thisTerm.utf8ToString();
	                    DocsEnum docsEnum = termsEnum.docs(null, null);
	                    while ((docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
//	                        System.out.println("termText:" + termText + " TF:  " + 1.0 * docsEnum.freq() / terms.size());
	                    	System.out.println("termText:" + termText + " 频数:  " + 1.0 * docsEnum.freq());
	                    	
	                    	HashMap<String,Object> term = new HashMap<String,Object>();
	                    	term.put("name", termText);
	                    	term.put("size", 1.0 * docsEnum.freq());
	                    	termlist.add(term);
	                    	
	                    	//{"name": "flare","children": [ {"name": "analytics","children": [      ]}]}
	                    }
	                }
	                JSONArray json = JSONArray.fromObject(termlist);
                	hashmap.put("children", json);
                	
	                jsonlist.add(hashmap);
//	                jsonall.putAll(hashmap);
	            }
	            JSONArray jsonarry = JSONArray.fromObject(jsonlist);
//	            jsonall.putAll(hashmap);
	            File file = new File("e:/filename.json");
	            // if file doesnt exists, then create it
	            if (!file.exists()) {
	             file.createNewFile();
	            }
	            FileWriter fw = new FileWriter(file.getAbsoluteFile());
	            BufferedWriter bw = new BufferedWriter(fw);
	            bw.write("{\"name\": \"flare\",\"children\": [ {\"name\": \"analytics\",\"children\": " + jsonarry + "}]}");
	            bw.close();

	            reader.close();
	            directory.close();
	        } catch (Exception e) {
	            e.printStackTrace();
	        }
	}
	
	
}
